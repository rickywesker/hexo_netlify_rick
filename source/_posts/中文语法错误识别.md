---
title: 中文语法错误识别
date: 2021-11-20 12:12:34
tags:
  - 项目
  - 神经网络
categories:
  - 知识蒸馏
---

<img src="https://pbs.twimg.com/media/D6W4XygUcAEEVza.png" alt="pas"  />

# 中文语法错误语句识别

# Motivation

对于母语非中文的外国人来讲，学习中文是一件很难的事情，先不论发音、拼写，语法上的组合变化就千变万化，一个细微的用词不当，整句话的意思就变了。这也是中文这门语言在NLP上遇到的一大难题之一，包括词语的引申意义、词语的多义性等。我们这次就来利用一些简单和稍微前沿（也不那么前沿x）的模型来做一个语法错误检测。这个任务可以归类为**文本分类任务**，就是说给出一个句子，判断此句子是否含有语法错误。比较进阶且有实用价值的是 **语法纠正** 任务，会用到常用的seq2seq模型。

我们来看看一些常见的语法错误，

。。。

作为一个中文不好的人，举例失败了。

在后面查看数据集的时候，我甚至觉得全部都没错x

# Before we start..

### 我们如何表示一个单词的含义?

早期的做法是我们建立所有**同义词synonym和下义词hypernym**(即“is a"的关系）的词库，比如WordNet，一个单词的含义就由它的同义词集合和下义词集合来定义。这一表示方法有很多问题，比如一个单词只在某些语境下和另一个词为同义词而其他语境下不是，词汇的新的含义很难包含进入词库，定义比较主观且需要较多人力整理，而且也很难量化两个词的相似程度。

那我们能否用向量来定量表示单词呢？一个简单的方法是我们用one-hot的向量来表示单词，即该单词对应所在元素为1，向量中其他元素均为0，如下所示：

![onehot](https://pic4.zhimg.com/80/v2-d2e29960eaea4ef7a5952a26acf7a8ff_720w.jpg)

而向量的维度就等于词库中的单词数目。

一个显然的问题是由于所有向量都是**互相正交**的，我们无法有效的表示两个向量间的相似度，并且向量维度过大。

对此，语言学家们有一个敏锐的观察：一个单词的含义往往是由经常和它一起出现的**附近的单词**们决定的，有点物以类聚，人以群分的感觉。即我们定义一个以该词为中心的定长的窗口，窗口内的其他词构成了它的上下文context，而通过这些context我们可以建立对该词的有效的表示。如下图中banking附近常出现的词决定了banking的含义。

![](https://pic4.zhimg.com/80/v2-87e6072f2ce18a031e05ae33ab3476c3_720w.jpg)

那么我们如何得到word vector呢？这一讲介绍的是一种较为流行的方法Word2Vec，由谷歌的NLP专家在2013年左右提出，其核心思想就是已知我们有了很大的文本库(corpus of text)，当我们用固定窗口不断的扫过文本库的句子时，我们有位于中间的center word c及其周边的单词们context words o, 而它们的相似度可用给定c的情况下o的条件概率来表示，我们不断的调整word vector使得这个概率最大化。每个单词w可用两个向量表示，一个是它作为center word时的向量 ![[公式]](https://www.zhihu.com/equation?tex=v_w) 以及它作为context word时的向量 ![[公式]](https://www.zhihu.com/equation?tex=u_w) 。

如下图所示，就是窗口大小为2，center word为into的context word的概率表示。

![](https://pic2.zhimg.com/80/v2-8a5e7fb209f558dcc2768c37969be549_720w.jpg)

在word2vec中，条件概率写作context word与center word的点乘形式再对其做softmax运算，即

![](https://pic3.zhimg.com/80/v2-1e786fb940528b712b11c1b9ec8cd5da_720w.jpg)

<img src="https://pic2.zhimg.com/80/v2-53fbb6c34874b1965fd01b6a0c4703c9_720w.jpg" style="zoom:50%;" />

而我们的目标函数或者损失函数就可以写作如下形式：

<img src="https://pic4.zhimg.com/80/v2-634afe8a6d0216e8e9367b4f003f4f93_720w.jpg" style="zoom:50%;" />

其中log形式是方便将联乘转化为求和，负号是希望将极大化似然率转化为极小化损失函数的等价问题。

有了目标函数以及每个条件概率的表现形式，我们就可以利用梯度下降算法来逐步求得使目标函数最小的word vector的形式了。

这里的word2vec算法又被叫做Skip-Gram model，还有另一种word2vec算法是Continuous Bag of Words，简称CBOW，它们的原理区别是Skip-Gram是求context word相对于center word的条件概率，而CBOW是求center相对于context word的条件概率，其他方面基本类似。

## 任务

1. 针对给定语句，识别该语句是否包含错误。

2. 使用提供的数据集（见附件， 数据来源NLPCC 2018 shared task：Grammatical Error Correction ），基于训练集训练分类模型，在测试集上进行性能测试。

3. 训练集共590000对正错句子对，文件train.cor为正确句子，文件train.err为错误句子，正确句子为错误句子的人工纠正结果。

4. 文件test.txt为测试集，共8229条测试样例，行首为正错标签，0表示句子错误，1表示句子正确。

5. 可实现或调用已有的分类算法，禁止在互联网通过匹配或检索的方式实现。

6. 对分类结果要进行分析。

### 分析

- 是次作业使用基础神经网络模型以及BERT。
- 在文本的处理中，一般首先是要对文本进行分词，然后构建词库，再将token映射为ID。
- 当在使用预训练bert时，由于词库大小已经固定（中文bert一般为21168），那么使用者需要做的只是将文本进行分词，然后利用bert固定词库将切分好的token映射为对应的ID。Bert分词起最主要功能的两个类分别为BasicTokenizer和WordpieceTokenizer，FullTokenizer类则将上述两个类结合起来。首先BasicTokenizer会先进行一序列的基本操作（转unicode -> 去除奇怪字符 -> 处理中文 -> 空格分词 -> 去除多余字符和标点分词）
- 对于中文来说，目前就是大多数采用的就是字粒度，即一个中文一个token。

### 数据预处理

```python
#Input
with open('train.cor') as f:
    data_ls_corr = f.readlines()
with open('train.err') as f:
    data_ls_err = f.readlines()

#Format
data_ls_err = list(map(lambda x:x.replace('\n',''),data_ls_err))
data_ls_cor = list(map(lambda x:x.replace('\n',''),data_ls_corr))
data_ls_cor = list(map(lambda x:x+'\t1\n',data_ls_cor))
data_ls_err = list(map(lambda x:x+'\t0\n',data_ls_err))
train_word = data_ls_cor+data_ls_err

#split validation and train set
random.shuffle(train_dat)
dev_word = train_word[:18000]
train_word = train_word[18000:]

#test set
ftest = open('test.txt','r')
test_list = ftest.readlines()
test_list = list(map(lambda x:x.replace('\n','\t'+x[0]+'\n')[2:],test_list))
```

最后输出的三个文件都是长这样的，

![image-20211120093643879.png](https://i.loli.net/2021/11/20/hvwVK6UESmxXa1b.png)

接下来就可以训练了。

对于字/词的embedding，我们使用自己训练以及预训练好的word2vec；对于bert，我们使用其自己的token方法

## 模型训练

### Attention + bilstm

- 要对词进行padding或者裁剪。
- 此次作业使用句子长度的平均值20作为padding_size

#### Attention

**Attention机制**：又称为注意力机制，顾名思义，是**一种能让模型对重要信息重点关注并充分学习吸收的技术**，它不算是一个完整的模型，应当是一种技术，能够作用于任何序列模型中。

对于一段文本序列，我们通常要使用某种机制对该序列进行编码，通过降维等方式将其encode成一个固定长度的向量，用于输入到后面的全连接层。

![img](https://pic4.zhimg.com/80/v2-43666630a1c31f6384bd3450fff49a3f_720w.jpg)

一般我们会使用CNN或者RNN（包括GRU或者LSTM）等模型来对序列数据进行编码，然后采用各种pooling或者对RNN直接取最后一个t时刻的hidden state作为句子的向量输出。这里会有一个问题：

常规的编码方法，无法体现对一个句子序列中不同语素的关注程度，在自然语言中，一个句子中的不同部分是有不同含义和重要性的，比如上面的例子中：I hate this movie.如果做情感分析，明显对hate这个词语应当关注更多。当然是用CNN和RNN能够编码这种信息。但是如果序列长度很长的情况下，这种方法会有一定的瓶颈。

常规的编码方法，无法体现对一个句子序列中**不同语素的关注程度**，在自然语言中，一个句子中的不同部分是有不同含义和重要性的

#### 本次作业任务的目标-文本分类

对于文本分类的**self-attention**思想很简单，而且计算成本相对来说不高：

Query和Key，value都是相同的，即输入的句子序列信息（可以是词向量lookup后的序列信息，也可以先用cnn或者rnn进行一次序列编码后得到的处理过的序列信息。）后面的步骤与上述的都是一样的：

1、首先建立句子序列中的每个词 ![[公式]](https://www.zhihu.com/equation?tex=q_i) 与句子其他词k的注意力权重 ![[公式]](https://www.zhihu.com/equation?tex=a_i)

2、然后将注意力权重向量进行softmax归一化，并与句子序列的所有时刻的信息（词向量或者rnn hidden state）进行线性加权。

![img](https://pic3.zhimg.com/80/v2-80790364856cb3e41f9c31b2ecb682fe_720w.jpg)

本次作业中使用BiLSTM + Attention作为组合进行实验，以下为加入早停机制以及不加入早停机制的训练结果。

![image-20211120093643879.png](https://i.loli.net/2021/11/20/hvwVK6UESmxXa1b.png)

### BERT

BERT的全称为Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型。它强调了不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的**masked language model（MLM）**，以致能生成**深度的双向**语言表征。

该模型有以下主要优点：

**1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。**

**2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。**

以往的预训练模型的结构会受到单向语言模型*（从左到右或者从右到左）*的限制，因而也限制了模型的表征能力，使其只能获取单方向的上下文信息。而BERT利用MLM进行预训练并且采用深层的双向Transformer组件*（单向的Transformer一般被称为Transformer decoder，其每一个token（符号）只会attend到目前往左的token。而双向的Transformer则被称为Transformer encoder，其每一个token会attend到所有的token。）*来构建整个模型，因此最终生成**能融合左右上下文信息**的深层双向语言表征。

![img](https://pic3.zhimg.com/80/v2-f0618dc2c2f62bd8d71c2195947be1d6_720w.jpg)

#### Bert的选择

是次作业会使用 https://huggingface.co/bert-base-chinese 的bert。

```python
class Model(nn.Module):

    def __init__(self, config):
        super(Model, self).__init__()
        self.bert = BertModel.from_pretrained(config.bert_path)
        for param in self.bert.parameters():
            param.requires_grad = True
        self.fc = nn.Linear(config.hidden_size, config.num_classes)

    def forward(self, x):
        context = x[0]  # 输入的句子
        mask = x[2]  
        # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]
        _, pooled = self.bert(context, attention_mask=mask, output_all_encoded_layers=False)
        out = self.fc(pooled)
        return out
```

## 实验结果

本次实验使用pytorch作为深度学习框架，显卡为RTX3070

# Result

| Model                  | Acc        | LR       | Epoch | Pad_size | Dropout |
| ---------------------- | ---------- | -------- | ----- | -------- | ------- |
| Bilstm+Attention       | 63.15%     | 1e-3     | 10    | 20       | 0.5     |
| Bilstm+Attention(Word) | 61.95%     | 1e-3     | 10    | 20       | 0.5     |
| **BERT**               | **70.38%** | **5e-5** | **3** | **20**   | **/**   |
| CNN                    | 53.78%     | 1e-3     | 10    | 20       | 0.5     |
|                        |            |          |       |          |         |

普遍来讲，利用字粒度能更好地捕捉“语法错误”这一类特征，因为直观上来讲语法错误的语义其实占比不重，通常是用字顺序，所以用词粒度的效果会比较差。

## 改进空间

- 超参调整
  - 学习率调整策略
  - 不同的权重初始化（本次实验使用Xavier）
  - pad size调整
  - 正则化的处理
- 数据增强
